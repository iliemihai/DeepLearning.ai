
C1W2L02
-------
House price prediction linear function is a neural network with one neuron:

a)
           X  -
             -
           X-  X
         X -            -------> this is RELU function
          -  X
----------

b)
----------
size     |-
         | -
         |  ->O -
         | -     -
#bedrooms|-       ->O -
         | -    -       - - ->-------
         |  ->O --->O - - - ->|price|
         | -    -       - - ->-------
zip code |-      -->O -          |
         | -    -                V
         |  ->O                  y
         | -
wealth   |-
---------
    | 
    V
    x

C1W2L03
-------
Supervised learning
ffn , cnn, rnn

Structured data vs unstructured data

C1W1L04
-------

Train bigger network + add more labelled data to obtain better results

For smaller sets we cannot see which algorithm performs best

Switching from sigmoid to relu (in sigmoid function gradients at end intervals are 0, so the training is very slow comparative to relu)


C1W2L01
-------

Logistic Regression = function used for Binary Classification


RGB matrix representation of image

RED 64X64
--             -- 
|255  231  41  22|
|232 321 321   33|
|323 323 312   32|
| 3  321  321   2|
|3213 3  321    3|
|32 3213  312 321|
|32  321  331 321|
--             --                      -   -
GREEN  64X64                           |255| 
--             --                      |231|
|155  31  141  122|                    | : |
|232 321 321   33|                     | : |
|323 323 312   32|                     |155|
| 3  321  321   2|                     |31 |
|3213 3  321    3|    ============> x =| : |
|32 3213  312 321|                     | : |
|32  321  331 321|                     |255|
--             --                      |231|
BLUE   64X64                           | : |
--             --                      | : |
|255  231  41  22|                     -   -
|232 321 321   33|                  x= 64X64X3
|323 323 312   32|
| 3  321  321   2|
|3213 3  321    3|
|32 3213  312 321|
|32  321  331 321|
--             --


(x, y) x = R^3, y = {0,1}



n training examples: {(x^1, y^1), (x^2, y^2), ....}

    [             ]
    [ |  |       |]
    [ |  |       |]
X = [x^1 x^2...x^n]     Y = [y^1 y^2 ... y^2]
    [ |  |       |]     Y shape = (1, n)
    [ |  |       |]
    [             ]

X = R^n*m
X shape = (n, m)


C1W2L02
-------
              ^
Given x, want y = P(y=1|x)

x = R^n

Parameter: w = R^n, b=R
        ^
Output: y =sigm(wTx + b)  where   sigm(z) = 1/(1+e^(-z))


C1W2L03
-------

z^i = wTx^i + b
            ^        ^
Loss : L(y, y) = 1/2(y-y)^2    this loss function is not convex
            ^           ^               ^
       L(y, y) = -(y*logy + (1-y)*log(1-y))


If y = 1 :             ^                   ^                 ^
       L(y, y) = -logy   <------ want logy to be large, want y to be large

If y = 0 :              ^                       ^
     ^  L(y, y) = -log(1-y)   <------ want log(1-y) to be large, want y to be small

                                   ^                     ^               ^
Cost function: J(w, b) = 1/m*SUM L(y, y) = -1/m*SUM[y*logy + (1-y)*log(1-y))]

C1W2L04
-------

Find w,b that minimize J(w, b)


Convex vs non convex functions

Repeat { w := w - a*dJ(w,b)/dw 
         b := b - a*dJ(w,b)/db }  a = learning rate

C1W2L05
-------

Derivatives: very intuitive explanation from Andrew Ng :D

f(a) = 3*a -> f'(a) = 3

C1W2L06 + C1W2L07
-----------------

f(a) = a^2 -> f'(a) = 2*a

C1W2L08
-------

Probably the most interesting for deeply understand backpropagation

Computation Graphs

J = 3 * v
v = a + u
u = b * c
a = 5
b = 3
c = 2

dJ/dv = 3
v = 11 -> v = 11.001
J = 33 -> J = 33.003

? dJ/da = ?
R :
a = 5 -> a = 5.001
v = 11 -> v = 11.001
J = 33 -> J = 33.003
dJ/da = dJ/dv * dv/da |-> dJ/da = dJ/dv*dv/da = 3 * 1 = 3
dv/da = 1             |

dFinalOutputVariable/dvar = dJ/dvar


dJ/da = 3 * 1 = dJ/dv * dv/du


dJ/db = dJ/du * du/db =  3 * 2 = 6			b = 3 -> 3.001
					   	     				u = b * c = 6 -> 6.002
					   	     				J = 33.006

dJ/dc = dJ/du * du/dc = 3 * 3 = 9

C1W2L09
-------

Logistic regression recap
z = w^T * x + b
y' = a = sig(z)
L(a, y) = - (ylog(a) + (1-y)log(1-a))

x1
w1
x2     z = w1 * x1 + x2 * x2 + b y' = a = sig(z) -> L(a,y)
w2
b


dL(a,y)/da = -y/a + (1-y)/(1-a)

dL(a,y)/dz = dL/da * da/dz = a * (1-a) *(-y/a + (1-y)/(1-a)) = a - y

dL/dw1 = x1 * dL(a,y)/dz = x1 * (a-y)
dL/dw2 = x2 * dL(a,y)/dz = x2 * (a-y)
dL/db = dz = a - y

w1 := w1 - alpha * dL/dw1
w2 := w2 - alpha * dL/dw2
b := b - alpha * dL/db

C1W2L10
-------

Logistic regression over m examples
J(w,b) = 1/n SUM L(ai,y)
ai = y'i = sig(w^T*xi + b)

d/dw1 J(w,b) = 1/n SUM d/dwi L(ai, yi)

J = 0, dw1=0, dw2=0, db=0
#first for loop
for i=1 to n
    zi = w^T * xi + b
    ai = sig(zi)
    Jt = -[yi log ai + (1-yi) log (1-ai)]
    #second for loop because we have n wi variables
    dzi = ai-yi
    dw1 += x1i dzi
    dw2 += x2i dzi
    db += dzi
J /= n
dw1 /= n
dw2 /= n
db /= n

dw1 = dJ/dw1

w1 := w1 - alpha*dw1
w2 := w2 - alpha*dw2
b := b - alpha*db

Problem is that we need 2 for loops -> need vectorization

C1W2L11
-------

z = w^T * x + b   w=[column vector] x = [column vector]

Non vectorize
z = 0
for i in range(n-x):
    z += w[i]*x[i]
z+= b

Vectorized
z = np.dot(w, x) + b

GPU, CPU -> SIMD instructions

C1W2L12
-------

u = A*v
ui = SUM Aij * vj
u = np.zeros()
for i...
    for j...
    	u[i] += A[i][j] * v[i]

vs 

u = np.dot(A,v)


v = [v1, v2,...]   u = [e^v1, e^v2,....]

u = np.zeros((n,1))
for i in range(n):
    u[i] = math.exp(v[i])

vs 

u = np.exp(v)

In logistic regression from C1W2L10 we will replace dw1=0, dw2=0,... with dw = np.zeros() vector and for dw1 += x1i dzi, dw2 += x2i dzi replace with one dw += xi * dzi vector

C1W2L13
-------

Z = [z1, z2, ...] = w^T * X + [b, b, ...] = [w^T * x1, w^T * x2, ...]

Z = np.dot(w^T, X) + b

C1W2L14
-------

Vectorize logistic regression

dz1 = a1 - y1 
dz2 = a2 - y2
...


dZ = [dz1, dz2, ...]

A = [a1, a2, ...]
Y = [y1, y2, ...]
dZ = A - Y

dW = 0          db = 0
dw += x1dz1     db += dz1
dw += x2dz2     db += dz2

dw /= n         db /= 2


Vectorize 

db = 1/n * SUM dzi = 1/n np.sum(dz)
dw = 1/n * X * dZ^T


for iteration in range(1000)
#gradient descent
Z = w^TX+b = np.sot(w^T,X)
A = sig(Z)
dZ = A-Y
dw = 1/nXdZ^T
db = 1/np.sum(dZ)
w := w-alpha*dw
b := b-alpha*db